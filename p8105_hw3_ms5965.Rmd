---
title: "p8105_hw3_ms5965"
output: github_document
---

#### Initial Set-up

```{r setup}
library(tidyverse)
library(p8105.datasets)
library(dplyr)
library(patchwork)
library(ggplot2)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1 

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.

Observations are the level of items in orders by user. There are user/order variables -- user ID, order ID, order day and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 


#### How many aisles, and which, are the most items ordered from?

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

#### Making a plot 

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  theme(axis.text.x = element_text(angle = -90, vjust = 0.5, hjust = 1))
```

#### Making a table

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  
knitr::kable()
```

#### Table for Pink Lady Apples and Coffee Ice Cream

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  
  mutate(order_dow = recode(order_dow,
                "0" = "Sunday", 
                "1" = "Monday",
                "2" = "Tuesday",
                "3" = "Wednesday",
                "4" = "Thursday",
                "5" = "Friday",
                "6" = "Saturday")) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

## Problem 2

### Part A

Importing, tidying, and describing the dataset. 

```{r}
accel_data = read_csv(file = "./data/accel_data.csv") %>% 
janitor::clean_names() 

tidy_accel_data = accel_data %>% 
  
pivot_longer(
  activity_1:activity_1440,
  names_to = "minutes",
  names_prefix = "activity_",
  values_to = "activity_count") %>% 
  
  drop_na(activity_count) %>% 
  
mutate(
  dow = recode(day,
               "Monday" = "weekday",
               "Tuesday" = "weekday",
               "Wednesday" = "weekday",
               "Thursday" = "weekday",
               "Friday" = "weekday",
               "Saturday" = "weekend",
               "Sunday" = "weekend"),
 
   day = factor(day, levels = c(
     "Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")),
  activity_count = as.numeric(activity_count),
  minutes = as.numeric(minutes))
```

This dataset has `r nrow(tidy_accel_data)` rows and `r ncol(tidy_accel_data)` columns. 

The dataset contains the following information, collected from an _accelerometer_, about the 65 year old male patient admitted at the Advanced Cardiac Care Center of Columbia University Medical Center:

*   Week number, day of the week and the day number since the start of the data collection. 

*   The observations under the activity_count variable are the number of activities per minute. 

*   The minutes variable represents each minute of a 24-hour day starting at midnight. 

### Part B

```{r}
tidy_accel_data %>% 
  group_by(week, day) %>% 
  mutate(total_per_day = sum(activity_count)) %>% 
  knitr::kable(digits = 0)  
```

The least amount of activity was in week four. The patient consistently does a higher amount of activity on Fridays and on the last two Saturdays, he had the least amount of activity. 

### Part C

```{r}
tidy_accel_data %>%
  group_by(day, minutes) %>% 
  
  ggplot(aes(x = minutes, y = activity_count, color = day)) +
  geom_line(alpha = .5)+
  geom_smooth(alpha = .5) + 
  
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440)) +
  
  labs(
    title = "24-hour activity time courses for each day",
    x = "Minutes",
    y = "Activity Count") + 
  
  theme(legend.position = "bottom") 
```

There is low activity on either extremes of the day - early morning or late at night. Majority of the activity is between 1000 and 1300 minutes, and between 480 and 720 minutes. 


## Problem 3

```{r}
data("ny_noaa")
```

These data were accessed from the NOAA National Climatic Data Center, and has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. 

This dataset has information on the weather station, data of observation, precipitation, snowfall, snow depth, and maximum and minimum temperatures. Greater volume of missing data will impact the accuracy of the final results, as it cam lead to bias - such as overestimation or underestimation. 


```{r}
tidy_ny_noaa = slice_tail(ny_noaa, n = 60000) %>% 
janitor::clean_names() %>% 
  
separate(date, into = c("year", "month", "day")) %>% 
  
  mutate(
    prcp = prcp * 0.1,
    tmax = as.numeric(tmax) * 0.1,
    tmin = as.numeric(tmin) * 0.1,
    snwd = as.numeric(snwd),
    year = as.numeric(year),
    month = as.numeric(month),
    day = as.numeric(day)
  )
```

### Part A

```{r}
snowfall_freq = tidy_ny_noaa %>% 
  count(snow, name = "n_snow") %>% 
  arrange(desc(n_snow)) %>% 
  select(n_snow, snow, everything()) %>% 
  slice_max(n_snow, n = 3) %>% 
knitr::kable(digits = 1)
```

The most common observed values for snowfall are 0mm, 25mm and those for which there are missing data for snowfall. 


### Part B

```{r}
tidy_ny_noaa %>% 
  filter(month %in% c("1", "7")) %>% 
  group_by(id, month, year) %>% 
  
  mutate(mean_tmax = mean(tmax), na.rm = TRUE) %>% 
  
  ggplot(aes(x = year, y = mean_tmax, color = id)) +
  geom_point() +
  geom_smooth() +
  facet_grid(~ month) +
  
labs(
  title = "Average maximum temperature in January and in July in each station across years.", 
  x = "Year", 
  y = "Average Maximum Temperature / C") +
  
theme(axis.text.x = element_text(angle = -90, vjust = 0.5, hjust = 1))

```

There is a marked increase in the avergae maximum temperature in July than in January, across all years from 1980 to 2010. There are some outliers, namely January 2004, when there was a drop in the temperature to below normal. 
